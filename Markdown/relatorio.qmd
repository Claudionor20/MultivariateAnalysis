---
title: "Análise Multivariada"
author: "Caio Falcão, Claudionor Ferreira, Guilherme Ceacero, Paulo Martinez"
toc: true
toc-title: "INDEX"
toc-location: left
format: 
  html:
    theme: default
    toc: true
    html-math-method: katex
    css: styles.css
    toc-location: left
editor_options: 
  markdown: 
    wrap: 72
---

Trabalho final da disciplina de Análise Multivariada, referente ao primeiro período de 2024. Esse trabalho é uma colaboração de Caio Falcão, Claudionor Ferreira, Guilherme Ceacero e Paulo Martinez. 

# Questão 1 


Nesta seção, demonstramos a direção discriminante de Fisher para discriminação linear entre duas populações. Consideramos duas populações $\pi_1$ e $\pi_2$ baseadas na amostra rotulada $( {(xj , \pi_i), \space j = 1, \ldots , n, \space i = 1, 2} )$, em que $n_1 + n_2 = n$ e $n_i$ é o número de elementos da população $\pi_i$.

## Considerações

### Definições

-   $\bar{x}_i$ representa o vetor de médias das observações em $\pi_i$.
-   $S_i$ representa a matriz de covariâncias amostral das observações em $\pi\_i$.
-   $S_p$ = $\frac{(n_1-1)S_1 + (n_2-1)S_2}{n-2}$ é a matriz de covariâncias combinada.

### Projeção

Para $\alpha, x \in \mathbb{R}^p$, definimos a projeção como:

$$
\begin{align}
 & z = \alpha^T x
\end{align}
$$

E consequentemente, as médias das populações $\pi_1$ e $\pi_2$ projetadas serão

$\hat m_1 = \alpha^T \space \bar{x}_1$

$\hat m_2 = \alpha^T \space \bar{x}_1$

E a diferença dessas projeções sendo

$\hat m_1 - \hat m_2 = \alpha^T(\bar{x}_1 - \bar{x}_2)$.

A variabilidade resultante na projeção é a variância da projeção $z$, que é dada por

$$
\begin{align}
    & Var(z) = Var(\alpha^T x) \nonumber \\
    & = \alpha^T \space Var(x) \space \alpha \nonumber \\
    & = \alpha^T \mathcal{S}_p \space \alpha \nonumber
\end{align}
$$

E então, $\phi = \left(\frac{\hat m_1 - \hat m_2}{S_z}\right)^2 = \left(\frac{\alpha^T (\bar{x}_1 - \bar{x}_2)}{\sqrt{\alpha^T S_p \space \alpha}}\right)^2$.

### Maximização da Distância

Queremos encontrar a direção $\alpha$ que maximiza a distância entre as médias projetadas das populações com relação à variabilidade resultante na projeção. Ou seja, queremos maximizar:

$$
\begin{equation}
\phi = \left(\frac{\alpha^T (\bar{x}_1 - \bar{x}_2)}{\sqrt{\alpha^T S_p \alpha}}\right)^2
\end{equation}
$$

## Demonstração

Vamos aplicar a restrição $\alpha^T S_p \space \alpha$ constante e igual à $1$, pois do contrário poderíamos ter $\phi$ tão grande quanto desejável. Nesse caso, então, vamos usar o método de Lagrange para otimização com restrição.

Começamos com a expressão a ser maximizada:

$$
\phi = \frac{(\alpha^T (\bar{x}_1 - \bar{x}_2))^2}{\alpha^T S_p \space \alpha}
$$

Para maximizar $\phi$, usamos multiplicadores de Lagrange. Definimos a função Lagrangiana:

$$ 
\mathcal{L}(\alpha, \lambda) = (\alpha^T (\bar{x}_1 - \bar{x}_2))^2 - \lambda (\alpha^T S_p \space \alpha - 1)
$$

Tomamos a derivada em relação a $\alpha$ e a igualamos a zero:

$$
\frac{\partial \mathcal{L}}{\partial \alpha} = 2 (\bar{x}_1 - \bar{x}_2) (\alpha^T (\bar{x}_1 - \bar{x}_2)) - \lambda 2 S_p \alpha = 0
$$

E agora, para $\lambda$:

$$
\frac{\partial \mathcal{L}}{\partial \lambda} = \alpha^T S_p \space \alpha - 1
$$

O que nos fornece então o seguinte sistema de equações:

$$
\begin{cases}
\frac{\partial \mathcal{L}}{\partial \alpha} = 2 (\bar{x}_1 - \bar{x}_2) (\alpha^T (\bar{x}_1 - \bar{x}_2)) - \lambda 2 S_p \space \alpha = 0 \\
\\
\frac{\partial \mathcal{L}}{\partial \lambda} = \alpha^T S_p \space \alpha - 1
\end{cases}
$$

Da primeira equação, temos que

$$
(\bar{x}_1 - \bar{x}_2) = \lambda S_p \space \alpha
$$

Para encontrar $\lambda$, substituímos $\alpha$ na segunda equação de restrição:

$$
\begin{equation}
(\bar{x}_1 - \bar{x}_2) = \lambda S_p \space \alpha \implies \alpha = \frac{S_p^{-1}(\bar{x}_1 - \bar{x}_2)}{\lambda}
\end{equation}
$$

A partir da restrição $\alpha^T S_p \space \alpha = 1$, temos:

$$
\begin{align}
 & \left(\frac{S_p^{-1}(\bar{x}_1 - \bar{x}_2)}{\lambda}\right)^T S_p \left(\frac{S_p(\bar{x}_1 - \bar{x}_2)}{\lambda}\right) = 1 \nonumber \\
 & \frac{1}{\lambda^2}(\bar{x}_1 - \bar{x}_2)^T S_p^{-1} \space S_p S_p^{-1} (\bar{x}_1 - \bar{x}_2) = 1 \nonumber \\
 & \lambda^2 = (\bar{x}_1 - \bar{x}_2)^T S_p^{-1} (\bar{x}_1 - \bar{x}_2) \nonumber \\
 & \lambda = \sqrt{(\bar{x}_1 - \bar{x}_2)^T S_p^{-1} (\bar{x}_1 - \bar{x}_2)}
\end{align}
$$

E agora, substituindo $\lambda$ encontrado em (4) na equação de $\alpha$ (3), temos:

$$
\begin{equation}
\alpha = \frac{S_p^{-1}(\bar{x}_1 - \bar{x}_2)}{\sqrt{(\bar{x}_1 - \bar{x}_2)^T S_p^{-1} (\bar{x}_1 - \bar{x}_2)}}
\end{equation}
$$

Note que o denominador dessa equação é um escalar $1x1$, e portanto, a direção de $\alpha$ que maximiza a diferença das duas populações é dada pelo vetor $\alpha = S_p^{-1}(\bar{x}_1 - \bar{x}_2)$ pelo método de otimização restrita de Lagrange.

## Conclusão

Dessa forma, mostramos que a direção $\alpha$ que maximiza a distância entre as médias projetadas das populações com relação à variabilidade resultante na projeção é a direção discriminante de Fisher.


# Questão 2 

Após realizar a importação da base de dados, verificamos se a base possui valores faltantes e se as variáveis estão corretamente formatadas: 


```{r, warning=FALSE}
library(candisc)
data(Wolves)
head(Wolves)
naniar::miss_var_summary(Wolves)
str(Wolves)
```

Com a base devidamente importada e sem a presença de valores faltantes, prosseguimos com a análise descritiva:

## Análise descritiva: 

Primeiro, verificamos as variáveis qualitativas da base de dados: 
```{r grupos}
library(ggplot2)
table(Wolves$sex)
table(Wolves$location)

Wolves |> ggplot(aes(x = group))+
  geom_bar(fill = "#5A639C", width = .5)+
  labs(y = "Frequência", x = "Grupo")+
  theme_minimal()
```
Observa-se que o grupo mais presente na amostra são de lobos machos da localização "ar".

Para as variáveis quantitativas, considere as seguintes medidas de resumo:

```{r medidas}
summary(Wolves[,4:12]) ### Melhorar essas medidas -> dp, cv, assimetria
```

Além das medidas resumos, pode ser de interesse realizar uma análise gráfica nos dados. Para isso, veja o histograma das variáveis: 

```{r histograma}
par(mfrow = c(3,3))
str(Wolves)
for(i in 1:ncol(Wolves)){
  name = names(Wolves)[i]
  if(is.numeric(Wolves[[i]])){
    label = attr(Wolves[[i]], "label")
    main_title = ifelse(!is.null(label), label, name)
    
    hist(Wolves[[i]], xlab= "", main = main_title)
  }
}
```
Surge também, a necessidade de verificar a correlação entre as variáveis. 

```{r correlacao}
pairs(Wolves[,4:12], 
      pch=21)

corrplot::corrplot(cor(Wolves[,4:12]), method = "square" , order = "hclust", tl.col='black', tl.cex=.75) 

```


Por fim, considere o boxplot das variáveis separadas por grupo. 

```{r boxplot}
plots <- list()
for (i in 1:ncol(Wolves)) {
  var_name = names(Wolves)[i]

  if (is.numeric(Wolves[[var_name]])) {
    label <- attr(Wolves[[var_name]], "label")
  
    p <- ggplot(data = Wolves, aes_string(x = "group", y = var_name)) +
      geom_boxplot() +
      labs(title = label, x = "", y = "") +
      theme_minimal()
    
    plots[[length(plots) + 1]] <- p
  }
}
library(gridExtra)
do.call("grid.arrange", c(plots, ncol = 3))
```

## Análise de discriminante

odod\

## K-means

```{r ncluster}
#### K-means ####
library(factoextra)

X = Wolves[,-c(1:3)] #Sem considera grupo -< mas deveriamos considerar sexo e loc??

X = as.matrix(X)

fviz_nbclust(X, kmeans, method = "wss")
```
Pelo gráfico parece razoável escolher 4 clusters
```{r clustering}

K = 4

km_res <- kmeans(X, centers = K)

# Plotar os clusters fornecidos pelo K-means
plot(X, col = km_res$cluster, pch = 16, main = "Clusters K-means")

# Plotar os centróides dos grupos
points(km_res$centers, col = 1:K, pch = 8, cex = 2)

# Plotar o gráfico de agrupamento usando fviz_cluster
fviz_cluster(km_res, data = X,
             ggtheme = theme_minimal(),
             main = "Gráfico de Agrupamento")
```

## Agrupamento hierárquico

manter ou nao as variáveis sexo e location 

# Questão 3 
```{r leitura3}
dados = read.table("https://raw.githubusercontent.com/Claudionor20/MultivariateAnalysis/main/bases/alimentos.txt", header = T)
row.names(dados)=dados$pais
head(dados)
```

## Hierarquia parcial

Em um primeiro momento, será de interesse construir um modelo hierárquico utilizando apenas as variáveis leite e fruta/legumes. 
```{r hir_leiteFL}
dados_LF = dplyr::select(dados, lech, fleg)
dados_LF = scale(dados_LF)
dist_matrix = dist(dados_LF)

hc_FL = hclust(dist_matrix, method = "complete")

plot(hc_FL, main = "Cluster usando Leite e Frutas/Legumes", sub = "", xlab = "")
```

## Hierarquia completa
```{r hier_ALL}
dados_All = dplyr::select(dados, -pais)
dados_All = scale(dados_All)
dist_matrix_All = dist(dados_All)

hc_All = hclust(dist_matrix_All, method = "complete")

```

## Hierarquica completa com average

```{r hier_avg}
Xa = dplyr::select(dados, -pais)
Xa = scale(Xa)
dist_matrix_avg = dist(Xa)

hc_avg = hclust(dist_matrix_avg, method = "average")

plot(hc_avg, main = "Average Linkage Hierarchical Clustering", sub = "", xlab = "")
```

## cortando as árvores

Cortando as árvores referente ao $complete \quad linkage$ com as variáveis leite e fruta/legumes

```{r cutTree complete}
#k=2
clustersFL_2 <- cutree(hc_FL, k = 2)
cluster_fl2 = data.frame(clustersFL_2)
cluster_fl2$cluster <- clustersFL_2
cluster_fl2$pais = dados$pais
#k = 3
clustersFL_3 <- cutree(hc_FL, k = 3)
cluster_fl3 = data.frame(clustersFL_3)
cluster_fl3$cluster <- clustersFL_3
cluster_fl3$pais = dados$pais
cluster_fl3

```



Cortando as árvores referente a $complete \quad linkage$ com todas variáveis

```{r cutTree avg}
#k = 2
clustersAll_2 <- cutree(hc_All, k = 2)
cluster_All2 = data.frame(clustersAll_2)
cluster_All2$cluster <- clustersAll_2
cluster_All2$pais = dados$pais
cluster_All2

#k = 3
clustersAll_3 <- cutree(hc_All, k = 3)
cluster_All3 = data.frame(clustersAll_3)
cluster_All3$cluster <- clustersAll_3
cluster_All3$pais = dados$pais
cluster_All3

```
## Visualização gráfico -> pesquisar sobre

```{r}
library(factoextra)

fviz_cluster(list(data = dados_LF, cluster = clustersFL_3),
             geom = "point",
             ellipse.type = "norm",
             main = "Cluster usando Leite e Frutas/Legumes")
```

# Questão 4 

### Bases
Considere as bases a serem utilizadas na análise

```{r bibliotecas4, warning=FALSE}
require(dplyr)
require(GGally)
library(ggplot2)
library(reshape2)
library(caret)
library(corrplot)
```

```{r basesIPS}
ips =  read.csv(file = "https://raw.githubusercontent.com/Claudionor20/MultivariateAnalysis/main/bases/IPS.csv", sep = ';')

head(ips)
```


## Análise Descritiva de Dados

### Média de IPS ao longo dos anos por região administrativa

```{r}
ips |>
  group_by(regiao_administrativa) %>%
  summarise(media_ips = mean(ips_geral)) %>%
  arrange(desc(media_ips)) %>%
  ggplot(aes(x = reorder(regiao_administrativa, media_ips), y = media_ips)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Média de IPS por Região Administrativa",
       x = "Região Administrativa",
       y = "Média de IPS")

```

### Evolução anual do IPS de cada regisão administrativa

```{r fig.width=15, fig.height=8}
ips |>
  ggplot(aes(x = factor(ano), y = ips_geral)) +
  geom_bar(stat = "identity", position = "dodge", aes(fill = regiao_administrativa), width = 0.7) +
  scale_fill_viridis_d() +  
  theme_minimal() +
  theme(
    text = element_text(size = 14),  
    axis.text.x = element_text(size = 12),  
    axis.text.y = element_text(size = 12),  
    legend.title = element_text(size = 12), 
    legend.text = element_text(size = 10),  
    panel.grid.major = element_line(color = "gray", linetype = "dotted"),  
    panel.grid.minor = element_blank()  
  ) +
  labs(
    title = "Evolução do IPS por Região Administrativa",
    x = "Ano",
    y = "IPS",
    fill = "Região Administrativa"
  ) +
  facet_wrap(~ regiao_administrativa, scales = "free_y", ncol = 6)  
```

### Boxplot dos indicadores de IPS por ano

```{r}
ips_geral <- ips|>
  select(ano,ips_geral)

ips_geral|>
  ggplot(aes(x = as.factor(ano), y = ips_geral)) +
  geom_boxplot(aes(fill = "Ano"), color = "gray50", outlier.color = "red", outlier.shape = 16, outlier.size = 3) +
  scale_fill_manual(values = "#66C2A5") + 
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),  
    panel.grid.minor = element_blank(),  
    legend.position = "none"  
  ) +
  labs(
    title = "Distribuição do IPS por Ano",
    x = "Ano",
    y = "IPS"
  )
```
### Histogramas

```{r fig.width=12, fig.height=8}
data_long <- melt(ips, id.vars = c("ano", "regiao_administrativa"))


ggplot(data_long, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  theme_minimal() +
  facet_wrap(~ variable, scales = "free_x") +
  labs(title = "Histogramas Dos Indicadores do IPS",
       x = "Valor",
       y = "Frequência")
```

### Análise de Correlação

```{r fig.width=12, fig.height=8}
correlacao <- cor(ips[,-c(1,2)])
corrplot(correlacao, method = "circle", tl.cex = 0.6)
```

### Variáveis de alta correlação com IPS

```{r}
correlacao_ips <- as.data.frame(correlacao)
correlacao_ips <- correlacao_ips[1,]

variaveis_correlacionadas_ips <- which(correlacao_ips > 0.7)
nomes_variaveis_correlacionadas_ips <- colnames(correlacao_ips)[variaveis_correlacionadas_ips]
print(nomes_variaveis_correlacionadas_ips)

# É possível perceber que a maioria dos indicadores de IPS possuem correlação alta com o próprio IPS

```
